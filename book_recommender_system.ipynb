{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234494c4-919a-48ce-accf-9c0dfec57415",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2b69cf4-50ed-4bef-8813-e43e714b3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30051c9-ac18-41ff-8a27-0bc907b390be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed Ratings Dataset: 532337 entries\n",
      "Unique Users: 16842, Unique Books: 28643\n"
     ]
    }
   ],
   "source": [
    "# Get the directory of each dataset \n",
    "path = os.path.dirname(os.path.abspath(\"Books.csv\"))\n",
    "book_path = path + \"/datasets/Books.csv\"\n",
    "users_path = path + \"/datasets/Users.csv\"\n",
    "ratings_path = path + \"/datasets/Ratings.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "books_dataset = pd.read_csv(book_path, low_memory=False)\n",
    "users_dataset = pd.read_csv(users_path, low_memory=False)\n",
    "ratings_dataset = pd.read_csv(ratings_path, low_memory=False)\n",
    "\n",
    "# Filter out users and books with fewer than 7 ratings\n",
    "user_counts = ratings_dataset['User-ID'].value_counts()\n",
    "valid_users = user_counts[user_counts >= 7].index\n",
    "\n",
    "book_counts = ratings_dataset['ISBN'].value_counts()\n",
    "valid_books = book_counts[book_counts >= 7].index\n",
    "\n",
    "# Filter ratings dataset\n",
    "ratings_dataset = ratings_dataset[ratings_dataset['User-ID'].isin(valid_users) & ratings_dataset['ISBN'].isin(valid_books)]\n",
    "\n",
    "# Print trimmed dataset summaries\n",
    "print(f\"Trimmed Ratings Dataset: {ratings_dataset.shape[0]} entries\")\n",
    "print(f\"Unique Users: {ratings_dataset['User-ID'].nunique()}, Unique Books: {ratings_dataset['ISBN'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b12e57-c97d-4ad9-8dba-056972a93400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in merged dataset: \n",
      "User-ID             0\n",
      "Location            0\n",
      "Age            134958\n",
      "ISBN                0\n",
      "Book-Rating         0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in books dataset: \n",
      "ISBN                   0\n",
      "Book-Title             0\n",
      "Book-Author            2\n",
      "Year-Of-Publication    0\n",
      "Publisher              2\n",
      "Image-URL-S            0\n",
      "Image-URL-M            0\n",
      "Image-URL-L            3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merge the ratings and users dataset together based on User-ID\n",
    "merged_data = users_dataset.merge(ratings_dataset, on='User-ID')\n",
    "\n",
    "# Check for missing values in each dataset\n",
    "missing_merged = merged_data.isnull().sum()\n",
    "missing_books = books_dataset.isnull().sum()\n",
    "print(\"Missing values in merged dataset: \")\n",
    "print(missing_merged, end=\"\\n\\n\")\n",
    "print(\"Missing values in books dataset: \")\n",
    "print(missing_books)\n",
    "\n",
    "# Remove rows where Book-Author and Publisher are null\n",
    "books_dataset.dropna(subset=['Book-Author', 'Publisher'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a801fb38-2377-48c1-b54a-4403b7ee4134",
   "metadata": {},
   "source": [
    "# Split the Data and Get Some Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3326eef2-71ad-454a-95d0-200793ea500c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Mean: 2.6563896409459247 \n",
      "\n",
      "User Mean:\n",
      " User-ID\n",
      "8         1.250000\n",
      "17        1.800000\n",
      "53        7.333333\n",
      "99        5.142857\n",
      "114       8.000000\n",
      "            ...   \n",
      "278774    0.000000\n",
      "278838    0.000000\n",
      "278843    2.181818\n",
      "278851    4.000000\n",
      "278854    5.833333\n",
      "Name: Book-Rating, Length: 16676, dtype: float64 \n",
      "\n",
      "Item Mean:\n",
      " ISBN\n",
      "0 907 062 008    3.750000\n",
      "000000000        0.666667\n",
      "0000000000       4.714286\n",
      "0002005018       5.200000\n",
      "0002005115       1.800000\n",
      "                   ...   \n",
      "9871138148       5.000000\n",
      "987932504        4.571429\n",
      "B00009EF82       2.285714\n",
      "B0000AA9IZ       1.500000\n",
      "M79702002        2.166667\n",
      "Name: Book-Rating, Length: 28640, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing\n",
    "train_data, test_data = train_test_split(merged_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate global mean rating, user mean rating and item mean rating\n",
    "global_mean = train_data['Book-Rating'].mean()\n",
    "user_means = train_data.groupby('User-ID')['Book-Rating'].mean()\n",
    "item_means = train_data.groupby('ISBN')['Book-Rating'].mean()\n",
    "\n",
    "# Print the means\n",
    "print(\"Global Mean:\", global_mean,\"\\n\")\n",
    "print(\"User Mean:\\n\",user_means,\"\\n\")\n",
    "print(\"Item Mean:\\n\",item_means,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae12279-8b7d-4c58-8bb4-0f86b0541bbe",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe32ea98-c519-47dc-b7b9-6e80f9f36754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nearest neighbors for KNN\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def collaborative_filtering(user_id, item_id, user_item_matrix, k=5):\n",
    "    if user_id not in user_item_matrix.index or item_id not in user_item_matrix.columns:\n",
    "        return global_mean\n",
    "    \n",
    "    user_index = user_item_matrix.index.get_loc(user_id)\n",
    "\n",
    "    # create user item matrix\n",
    "    \n",
    "    user_item_matrix = ratings_dataset.pivot(index='User-ID', columns='ISBN', values='Book-Rating').fillna(0)\n",
    "    \n",
    "\n",
    "    # train model (knn)\n",
    "    knn = NearestNeighbors(n_neighbors=k, metric='cosine')\n",
    "    knn.fit(user_item_matrix.values.T)\n",
    "\n",
    "    # find k nearest neighbors\n",
    "\n",
    "    distances, indices = knn.kneighbors(user_item_matrix.iloc[user_index].values.reshape(1, -1), n_neighbors=k)\n",
    "    neighbors_indices = indices.flatten()[1:]\n",
    "\n",
    "    # predict rating\n",
    "    neighbors_ratings = user_item_matrix.iloc[neighbors_indices][item_id]\n",
    "    neighbors_ratings = neighbors_ratings[neighbors_ratings.notnull()]\n",
    "    print(user_item_matrix.head(5))\n",
    "    return neighbors_ratings.mean() if not neighbors_ratings.empty else global_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3da3986",
   "metadata": {},
   "source": [
    "# Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "853a6a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books similar to 0195153448: ['0582280044', '0801304652', '0801319536', '0195210301', '019866172X', '0192814907', '0198601654', '0198721129', '019509199X', '0553257765', '0192177478', '0844255610', '0192892878', '0195089618', '0192813242', '0192800329', '0191012084', '0195215214', '0192861026', '0195123034']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Combine relevant text features\n",
    "books_dataset['combine'] = books_dataset['Book-Title'] + ' ' + books_dataset['Book-Author'] + ' ' + books_dataset['Publisher']\n",
    "\n",
    "# Create the TF-IDF matrix\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(books_dataset['combine'])\n",
    "\n",
    "# Function for efficient content-based filtering\n",
    "def content_based_filtering(item_id, tfidf_matrix=tfidf_matrix, top_n=20):\n",
    "    # Find the index of the item\n",
    "    try:\n",
    "        idx = books_dataset[books_dataset['ISBN'] == item_id].index[0]\n",
    "    except IndexError:\n",
    "        return []  # If the book ID is not found\n",
    "\n",
    "    # Compute cosine similarities for the target book on demand\n",
    "    target_vector = tfidf_matrix[idx]\n",
    "    cosine_similarities = cosine_similarity(target_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Get the indices of the top N similar books (excluding the target book itself)\n",
    "    similar_indices = cosine_similarities.argsort()[-(top_n + 1):-1][::-1]\n",
    "\n",
    "    # Return the top N similar books' ISBNs\n",
    "    return books_dataset.iloc[similar_indices]['ISBN'].tolist()\n",
    "\n",
    "# Example Usage\n",
    "example_book_id = books_dataset['ISBN'].iloc[0]  # Replace with a valid ISBN\n",
    "similar_books = content_based_filtering(example_book_id)\n",
    "print(f\"Books similar to {example_book_id}: {similar_books}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce68091-c561-4480-916e-697f0996b6b0",
   "metadata": {},
   "source": [
    "# Hybrid Weighted Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97de3cc9-c9eb-4ddb-8486-4c51e79addc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommender(user_id, item_id, user_means, item_means, user_item_matrix, weights=(0.3, 0.3, 0.4)):\n",
    "    global_pred = global_mean\n",
    "    user_pred = user_means.get(user_id, global_mean)\n",
    "    item_pred = item_means.get(item_id, global_mean)\n",
    "    cf_pred = collaborative_filtering(user_id, item_id, user_item_matrix)\n",
    "\n",
    "    # Weighted sum\n",
    "    final_pred = (weights[0] * global_pred + weights[1] * user_pred + weights[2] * item_pred + (1 - sum(weights)) * cf_pred)\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d9c28-dd55-4dce-b67c-9e44390d2d17",
   "metadata": {},
   "source": [
    "# Create Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671340f9-20d2-431e-bad4-e2a7436bbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_user_item_matrix(matrix, test_ratio=0.2):\n",
    "    \"\"\"Splits the matrix into training and testing data.\"\"\"\n",
    "    test_matrix = matrix.copy()\n",
    "    train_matrix = matrix.copy()\n",
    "\n",
    "    # Mask 20% of the ratings in the test set\n",
    "    for user in matrix.index:\n",
    "        non_zero_indices = matrix.loc[user, :].to_numpy().nonzero()[0]\n",
    "        test_indices = random.sample(list(non_zero_indices), int(len(non_zero_indices) * test_ratio))\n",
    "        train_matrix.loc[user, matrix.columns[test_indices]] = 0  # Remove ratings from training set\n",
    "\n",
    "    return train_matrix, test_matrix\n",
    "\n",
    "# Create the user-book matrix\n",
    "user_book_matrix = ratings_dataset.pivot_table(\n",
    "    index='User-ID', columns='ISBN', values='Book-Rating', fill_value=0\n",
    ")\n",
    "\n",
    "train_matrix, test_matrix = train_test_split_user_item_matrix(user_book_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787becbf-cad4-49e2-b353-3811cb188d4b",
   "metadata": {},
   "source": [
    "# Prediction Models and RMSE Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fb5509a-dd67-4243-a974-b1be4a70bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_global_mean(global_mean, test_matrix):\n",
    "    \"\"\"Predict all ratings using the global mean.\"\"\"\n",
    "    pred = test_matrix.copy()\n",
    "    pred[pred != 0] = global_mean  # Replace all non-zero values with the global mean\n",
    "    return pred\n",
    "\n",
    "def predict_user_mean(user_means, test_matrix):\n",
    "    \"\"\"Predict ratings using user means.\"\"\"\n",
    "    pred = test_matrix.copy()\n",
    "    for user in test_matrix.index:\n",
    "        pred.loc[user, :] = user_means.get(user, global_mean)\n",
    "    return pred\n",
    "\n",
    "def predict_item_mean(item_means, test_matrix):\n",
    "    \"\"\"Predict ratings using item means.\"\"\"\n",
    "    pred = test_matrix.copy()\n",
    "    for item in test_matrix.columns:\n",
    "        pred[item] = item_means.get(item, global_mean)\n",
    "    return pred\n",
    "\n",
    "def calculate_rmse(actual, predicted):\n",
    "    \"\"\"Calculate RMSE between actual and predicted ratings, replacing NaNs with 0.\"\"\"\n",
    "    # Flatten the matrices\n",
    "    actual_values = actual.to_numpy().flatten()\n",
    "    predicted_values = predicted.to_numpy().flatten()\n",
    "    \n",
    "    # Replace NaN values with 0 for both actual and predicted values\n",
    "    actual_values = np.nan_to_num(actual_values, nan=0)\n",
    "    predicted_values = np.nan_to_num(predicted_values, nan=0)\n",
    "    \n",
    "    # Return RMSE for valid values\n",
    "    return sqrt(mean_squared_error(actual_values, predicted_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17324e-c78b-4cbb-be5d-faacee4a3147",
   "metadata": {},
   "source": [
    "# Run Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "675d728f-ee05-4592-8925-bd342849ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Global Mean RMSE: 0.03507656554843828\n",
      "User Mean RMSE: 7.344924415308821\n",
      "Item Mean RMSE: 7.413210835632333\n"
     ]
    }
   ],
   "source": [
    "# Global Mean\n",
    "global_mean = train_matrix[train_matrix != 0].mean().mean()\n",
    "global_pred = predict_global_mean(global_mean, test_matrix)\n",
    "\n",
    "# User Mean\n",
    "user_means = train_matrix.replace(0, np.nan).mean(axis=1)\n",
    "user_pred = predict_user_mean(user_means, test_matrix)\n",
    "\n",
    "# Item Mean\n",
    "item_means = train_matrix.replace(0, np.nan).mean(axis=0)\n",
    "item_pred = predict_item_mean(item_means, test_matrix)\n",
    "\n",
    "# RMSE Calculation\n",
    "global_rmse = calculate_rmse(test_matrix, global_pred)\n",
    "user_rmse = calculate_rmse(test_matrix, user_pred)\n",
    "item_rmse = calculate_rmse(test_matrix, item_pred)\n",
    "\n",
    "# Results\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Global Mean RMSE: {global_rmse}\")\n",
    "print(f\"User Mean RMSE: {user_rmse}\")\n",
    "print(f\"Item Mean RMSE: {item_rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
