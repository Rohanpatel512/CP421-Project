{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234494c4-919a-48ce-accf-9c0dfec57415",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b69cf4-50ed-4bef-8813-e43e714b3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b30051c9-ac18-41ff-8a27-0bc907b390be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of books dataset\n",
      "         ISBN                                         Book-Title  \\\n",
      "0  0195153448                                Classical Mythology   \n",
      "1  0002005018                                       Clara Callan   \n",
      "2  0060973129                               Decision in Normandy   \n",
      "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
      "4  0393045218                             The Mummies of Urumchi   \n",
      "\n",
      "            Book-Author Year-Of-Publication                   Publisher  \\\n",
      "0    Mark P. O. Morford                2002     Oxford University Press   \n",
      "1  Richard Bruce Wright                2001       HarperFlamingo Canada   \n",
      "2          Carlo D'Este                1991             HarperPerennial   \n",
      "3      Gina Bari Kolata                1999        Farrar Straus Giroux   \n",
      "4       E. J. W. Barber                1999  W. W. Norton &amp; Company   \n",
      "\n",
      "                                         Image-URL-S  \\\n",
      "0  http://images.amazon.com/images/P/0195153448.0...   \n",
      "1  http://images.amazon.com/images/P/0002005018.0...   \n",
      "2  http://images.amazon.com/images/P/0060973129.0...   \n",
      "3  http://images.amazon.com/images/P/0374157065.0...   \n",
      "4  http://images.amazon.com/images/P/0393045218.0...   \n",
      "\n",
      "                                         Image-URL-M  \\\n",
      "0  http://images.amazon.com/images/P/0195153448.0...   \n",
      "1  http://images.amazon.com/images/P/0002005018.0...   \n",
      "2  http://images.amazon.com/images/P/0060973129.0...   \n",
      "3  http://images.amazon.com/images/P/0374157065.0...   \n",
      "4  http://images.amazon.com/images/P/0393045218.0...   \n",
      "\n",
      "                                         Image-URL-L  \n",
      "0  http://images.amazon.com/images/P/0195153448.0...  \n",
      "1  http://images.amazon.com/images/P/0002005018.0...  \n",
      "2  http://images.amazon.com/images/P/0060973129.0...  \n",
      "3  http://images.amazon.com/images/P/0374157065.0...  \n",
      "4  http://images.amazon.com/images/P/0393045218.0...  \n",
      "First 5 rows of users dataset\n",
      "   User-ID                            Location   Age\n",
      "0        1                  nyc, new york, usa   NaN\n",
      "1        2           stockton, california, usa  18.0\n",
      "2        3     moscow, yukon territory, russia   NaN\n",
      "3        4           porto, v.n.gaia, portugal  17.0\n",
      "4        5  farnborough, hants, united kingdom   NaN\n",
      "First 5 rows of ratings dataset\n",
      "   User-ID        ISBN  Book-Rating\n",
      "0   276725  034545104X            0\n",
      "1   276726  0155061224            5\n",
      "2   276727  0446520802            0\n",
      "3   276729  052165615X            3\n",
      "4   276729  0521795028            6\n"
     ]
    }
   ],
   "source": [
    "# Get the directory of each dataset \n",
    "path = os.path.dirname(os.path.abspath(\"Books.csv\"))\n",
    "book_path = path + \"/datasets/Books.csv\"\n",
    "users_path = path + \"/datasets/Users.csv\"\n",
    "ratings_path = path + \"/datasets/Ratings.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "books_dataset = pd.read_csv(book_path, low_memory=False)\n",
    "users_dataset = pd.read_csv(users_path, low_memory=False)\n",
    "ratings_dataset = pd.read_csv(ratings_path, low_memory=False)\n",
    "\n",
    "# Print the first 5 rows of each dataset \n",
    "print(\"First 5 rows of books dataset\")\n",
    "print(books_dataset.head(5))\n",
    "\n",
    "print(\"First 5 rows of users dataset\")\n",
    "print(users_dataset.head(5))\n",
    "\n",
    "print(\"First 5 rows of ratings dataset\")\n",
    "print(ratings_dataset.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dafe5802-fc02-4c87-9ff8-c15eb17080a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books Dataset Summary: \n",
      "              ISBN      Book-Title      Book-Author Year-Of-Publication  \\\n",
      "count       271360          271360           271358              271360   \n",
      "unique      271360          242135           102022                 118   \n",
      "top     020130998X  Selected Poems  Agatha Christie                2002   \n",
      "freq             1              27              632               17627   \n",
      "\n",
      "        Publisher                                        Image-URL-S  \\\n",
      "count      271358                                             271360   \n",
      "unique      16807                                             271044   \n",
      "top     Harlequin  http://images.amazon.com/images/P/155936078X.0...   \n",
      "freq         7535                                                  2   \n",
      "\n",
      "                                              Image-URL-M  \\\n",
      "count                                              271360   \n",
      "unique                                             271044   \n",
      "top     http://images.amazon.com/images/P/155936078X.0...   \n",
      "freq                                                    2   \n",
      "\n",
      "                                              Image-URL-L  \n",
      "count                                              271357  \n",
      "unique                                             271041  \n",
      "top     http://images.amazon.com/images/P/155936078X.0...  \n",
      "freq                                                    2  \n",
      "-----------------------\n",
      "Users Dataset Summary: \n",
      "            User-ID            Age\n",
      "count  278858.00000  168096.000000\n",
      "mean   139429.50000      34.751434\n",
      "std     80499.51502      14.428097\n",
      "min         1.00000       0.000000\n",
      "25%     69715.25000      24.000000\n",
      "50%    139429.50000      32.000000\n",
      "75%    209143.75000      44.000000\n",
      "max    278858.00000     244.000000\n",
      "-----------------------\n",
      "Ratings Dataset Summary:\n",
      "            User-ID   Book-Rating\n",
      "count  1.149780e+06  1.149780e+06\n",
      "mean   1.403864e+05  2.866950e+00\n",
      "std    8.056228e+04  3.854184e+00\n",
      "min    2.000000e+00  0.000000e+00\n",
      "25%    7.034500e+04  0.000000e+00\n",
      "50%    1.410100e+05  0.000000e+00\n",
      "75%    2.110280e+05  7.000000e+00\n",
      "max    2.788540e+05  1.000000e+01\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "# Display the statistical summary for each dataset\n",
    "print(\"Books Dataset Summary: \")\n",
    "print(books_dataset.describe())\n",
    "print(\"-----------------------\")\n",
    "\n",
    "print(\"Users Dataset Summary: \")\n",
    "print(users_dataset.describe())\n",
    "print(\"-----------------------\")\n",
    "\n",
    "print(\"Ratings Dataset Summary:\")\n",
    "print(ratings_dataset.describe())\n",
    "print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94b12e57-c97d-4ad9-8dba-056972a93400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in merged dataset: \n",
      "User-ID             0\n",
      "Location            0\n",
      "Age            309492\n",
      "ISBN                0\n",
      "Book-Rating         0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in books dataset: \n",
      "ISBN                   0\n",
      "Book-Title             0\n",
      "Book-Author            2\n",
      "Year-Of-Publication    0\n",
      "Publisher              2\n",
      "Image-URL-S            0\n",
      "Image-URL-M            0\n",
      "Image-URL-L            3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merge the ratings and users dataset together based on ID\n",
    "merged_data = users_dataset.merge(ratings_dataset, on='User-ID')\n",
    "\n",
    "# Check for missing values in each dataset\n",
    "missing_merged = merged_data.isnull().sum()\n",
    "missing_books = books_dataset.isnull().sum()\n",
    "print(\"Missing values in merged dataset: \")\n",
    "print(missing_merged, end=\"\\n\\n\")\n",
    "\n",
    "print(\"Missing values in books dataset: \")\n",
    "print(missing_books)\n",
    "\n",
    "# Remove the rows where Book-Author and Publisher are null\n",
    "books_dataset.dropna(subset=['Book-Author', 'Publisher'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a801fb38-2377-48c1-b54a-4403b7ee4134",
   "metadata": {},
   "source": [
    "# Split the Data and Get Some Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3326eef2-71ad-454a-95d0-200793ea500c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Mean: 2.8676801214145313 \n",
      "\n",
      "User Mean:\n",
      " User-ID\n",
      "7          0.000000\n",
      "8          2.090909\n",
      "9          2.000000\n",
      "10         3.000000\n",
      "12        10.000000\n",
      "            ...    \n",
      "278846     4.000000\n",
      "278849     2.250000\n",
      "278851     3.857143\n",
      "278852     8.000000\n",
      "278854     4.833333\n",
      "Name: Book-Rating, Length: 92906, dtype: float64 \n",
      "\n",
      "Item Mean:\n",
      " ISBN\n",
      " 0330299891    3.0\n",
      " 0586045007    0.0\n",
      " 9022906116    3.5\n",
      " 9032803328    0.0\n",
      " 9044922572    0.0\n",
      "              ... \n",
      "cn113107       0.0\n",
      "ooo7156103     7.0\n",
      "§423350229     0.0\n",
      "´3499128624    8.0\n",
      "Ô½crosoft      7.0\n",
      "Name: Book-Rating, Length: 298432, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing\n",
    "train_data, test_data = train_test_split(merged_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate global mean rating, user mean rating and item mean rating\n",
    "global_mean = train_data['Book-Rating'].mean()\n",
    "user_means = train_data.groupby('User-ID')['Book-Rating'].mean()\n",
    "item_means = train_data.groupby('ISBN')['Book-Rating'].mean()\n",
    "\n",
    "# Print the means\n",
    "print(\"Global Mean:\", global_mean,\"\\n\")\n",
    "print(\"User Mean:\\n\",user_means,\"\\n\")\n",
    "print(\"Item Mean:\\n\",item_means,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae12279-8b7d-4c58-8bb4-0f86b0541bbe",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe32ea98-c519-47dc-b7b9-6e80f9f36754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nearest neighbors for KNN\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def collaborative_filtering(user_id, item_id, user_item_matrix, k=5):\n",
    "    if user_id not in user_item_matrix.index or item_id not in user_item_matrix.columns:\n",
    "        return global_mean\n",
    "    \n",
    "    user_index = user_item_matrix.index.get_loc(user_id)\n",
    "\n",
    "    # create user item matrix\n",
    "    \n",
    "    user_item_matrix = ratings_dataset.pivot(index='User-ID', columns='ISBN', values='Book-Rating').fillna(0)\n",
    "    \n",
    "\n",
    "    # train model (knn)\n",
    "    knn = NearestNeighbors(n_neighbors=k, metric='cosine')\n",
    "    knn.fit(user_item_matrix.values.T)\n",
    "\n",
    "    # find k nearest neighbors\n",
    "\n",
    "    distances, indices = knn.kneighbors(user_item_matrix.iloc[user_index].values.reshape(1, -1), n_neighbors=k)\n",
    "    neighbors_indices = indices.flatten()[1:]\n",
    "\n",
    "    # predict rating\n",
    "    neighbors_ratings = user_item_matrix.iloc[neighbors_indices][item_id]\n",
    "    neighbors_ratings = neighbors_ratings[neighbors_ratings.notnull()]\n",
    "    print(user_item_matrix.head(5))\n",
    "    return neighbors_ratings.mean() if not neighbors_ratings.empty else global_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3da3986",
   "metadata": {},
   "source": [
    "# Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a6a04",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 549. GiB for an array with shape (271356, 271356) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m tfidf_matrix_svd \u001b[38;5;241m=\u001b[39m svd\u001b[38;5;241m.\u001b[39mfit_transform(tfidf_matrix)  \u001b[38;5;66;03m# Apply SVD\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarity (return sparse matrix to save memory)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m cosine_sim \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_matrix_svd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf_matrix_svd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Function to get top-N similar books\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontent_based_filtering\u001b[39m(item_id, cosine_sim\u001b[38;5;241m=\u001b[39mcosine_sim, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Find the index of the book in the dataset\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Isaac\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Isaac\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1687\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     Y_normalized \u001b[38;5;241m=\u001b[39m normalize(Y, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1687\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_normalized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
      "File \u001b[1;32mc:\\Users\\Isaac\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:205\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    203\u001b[0m         ret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a, b)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    208\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m ):\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 549. GiB for an array with shape (271356, 271356) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#import SVD to reduce dimensionality\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "#create content based filtering model\n",
    "books_dataset['combine'] = books_dataset['Book-Title'] + ' ' + books_dataset['Book-Author'] + ' ' + books_dataset['Publisher']\n",
    "\n",
    "# create tfidf vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix = tfidf.fit_transform(books_dataset['combine'])\n",
    "\n",
    "\n",
    "# calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix, dense_output=)\n",
    "\n",
    "def content_based_filtering(item_id, cosine_sim=cosine_sim):\n",
    "    \n",
    "    idx = books_dataset[books_dataset['ISBN'] == item_id].index[0]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:6]\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    return books_dataset['ISBN'].iloc[book_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce68091-c561-4480-916e-697f0996b6b0",
   "metadata": {},
   "source": [
    "# Hybrid Weighted Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de3cc9-c9eb-4ddb-8486-4c51e79addc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommender(user_id, item_id, user_means, item_means, user_item_matrix, weights=(0.3, 0.3, 0.4)):\n",
    "    global_pred = global_mean\n",
    "    user_pred = user_means.get(user_id, global_mean)\n",
    "    item_pred = item_means.get(item_id, global_mean)\n",
    "    cf_pred = collaborative_filtering(user_id, item_id, user_item_matrix)\n",
    "\n",
    "    # Weighted sum\n",
    "    final_pred = (weights[0] * global_pred + weights[1] * user_pred + weights[2] * item_pred + (1 - sum(weights)) * cf_pred)\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c231d-893b-4559-8dc4-58e02174fd78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
